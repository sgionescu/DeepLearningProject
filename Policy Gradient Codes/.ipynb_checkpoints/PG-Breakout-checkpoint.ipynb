{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-26 10:05:58,072] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode number: 310\n",
      "INFO:tensorflow:Restoring parameters from PG-Breakout-ckpt-1\\Breakout.ckpt-310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-12-26 10:05:58,796] Restoring parameters from PG-Breakout-ckpt-1\\Breakout.ckpt-310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from beginning\n",
      "episode number: 0\n",
      "\tep 1: reward: 2.0 duration: 259\n",
      "\tep 2: reward: 0.0 duration: 180\n",
      "\tep 3: reward: 0.0 duration: 170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-20cc5852a869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Uniformly pick an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mfeed_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mimage_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m82\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0maprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_aprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0maprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Epsilon greedy exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_3_6_gym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_3_6_gym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_3_6_gym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_3_6_gym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_3_6_gym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "\n",
    "# Initialize OpenAI for Breakout\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "\n",
    "# Hyper parameters for policy gradient model\n",
    "global num_actions, discount, batch_size\n",
    "num_actions = 3 # env.action_space.n # 6\n",
    "discount = 0.99\n",
    "save_every = 10\n",
    "train_every = 1\n",
    "reward_factor = 1\n",
    "max_episode = 100000 # When to terminate traininig\n",
    "initial_exp = -1 # Initial exploration probability\n",
    "final_exp = 0.01 # Final exploration probability\n",
    "\n",
    "# Hyper parameters for Network\n",
    "learning_rate=0.0005\n",
    "decay_ = 0.95\n",
    "epsilon_ = 1e-8\n",
    "FC_param = {'FC_1':600}\n",
    "\n",
    "# Initialize parameter for policy gradient model \n",
    "observation = env.reset()\n",
    "image_old  = None\n",
    "images, fake_labels, rewards_std, action_hist, reward_hist, reward_runn = [], [], [], [], [], []\n",
    "reward_episode = 0\n",
    "\n",
    "# Build network\n",
    "def Network(image_, FC_param):\n",
    "    \n",
    "    FC_tmp = tf.contrib.layers.flatten(image_, scope='Flatten')\n",
    "    \n",
    "    for key,value in FC_param.items():\n",
    "        FC_tmp = tf.layers.dense(inputs=FC_tmp, \n",
    "                                 units=value,\n",
    "                                 kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                     stddev=1./np.sqrt(5000), \n",
    "                                                                                     dtype=tf.float32),\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 use_bias=False,\n",
    "                                 name=key)\n",
    "    \n",
    "    logits = tf.layers.dense(inputs=FC_tmp, \n",
    "                             units=num_actions,\n",
    "                             kernel_initializer= tf.truncated_normal_initializer(mean=0,\n",
    "                                                                                 stddev=1./np.sqrt(500), \n",
    "                                                                                 dtype=tf.float32),\n",
    "                             use_bias=False,\n",
    "                             name='Logits')\n",
    "    \n",
    "    action_probs = tf.nn.softmax(logits, name='SoftMax')\n",
    "    \n",
    "    return action_probs\n",
    "\n",
    "# Get discounted reward and normalize it\n",
    "def discount_norm(rew):\n",
    "    rew_func = lambda a, v: a*discount + v # Reward function\n",
    "    \n",
    "    rew_reverse = tf.scan(rew_func, tf.reverse(rew,[True, False]))\n",
    "    discounted_rew = tf.reverse(rew_reverse,[True, False])\n",
    "    \n",
    "    mean, variance= tf.nn.moments(discounted_rew, [0])\n",
    "    discounted_rew -= mean\n",
    "    discounted_rew /= tf.sqrt(variance + 1e-6)\n",
    "    \n",
    "    return discounted_rew\n",
    "\n",
    "# Process image by cropping and binarizing\n",
    "def process_obs(obs):\n",
    "    obs = obs[32:196,8:152]\n",
    "    obs = obs[::2,::2,0]\n",
    "    obs[obs != 0] = 1 \n",
    "    return obs.astype(np.float)\n",
    "\n",
    "# Calculate running mean for plotting\n",
    "def get_running_mean(reward_hist):\n",
    "    \n",
    "    running_mean=[]\n",
    "    mean=0\n",
    "    for i in range(len(reward_hist)):\n",
    "        mean = 0.99*mean + 0.01*reward_hist[i]\n",
    "        running_mean.append(mean)\n",
    "        \n",
    "    return running_mean\n",
    "\n",
    "# Build model\n",
    "with tf.Graph().as_default() as g:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        image_ = tf.placeholder(dtype=tf.float32, shape=[None, 82, 72,1],name=\"image\")\n",
    "        fake_label_ = tf.placeholder(dtype=tf.float32, shape=[None, num_actions],name=\"fake_label\")\n",
    "        reward_ = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"reward\")\n",
    "        \n",
    "        # Get policy gradient\n",
    "        discounted_epr = discount_norm(reward_) \n",
    "\n",
    "        tf_aprob = Network(image_,FC_param)\n",
    "        loss = tf.nn.l2_loss(fake_label_-tf_aprob) # Define loss\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=decay_, epsilon=epsilon_)\n",
    "        \n",
    "        # Assign optimizer with artificial gradient\n",
    "        grads = optimizer.compute_gradients(loss, var_list=tf.trainable_variables(), grad_loss=discounted_epr)\n",
    "        \n",
    "        train_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "# Main program\n",
    "with g.as_default(), tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    \n",
    "    # Load from last session or start over\n",
    "    try:\n",
    "        episode_number = pickle.load(open('PG-Breakout-ckpt-1/last-episode.p','rb'))\n",
    "        print(\"episode number:\", episode_number)\n",
    "        saver.restore(sess, 'PG-Breakout-ckpt-1/Breakout.ckpt-'+str(episode_number))\n",
    "        reward_hist = pickle.load(open('PG-Breakout-ckpt-1/all_reward.p','rb'))\n",
    "        print('Continue from last saved episode')\n",
    "    except:\n",
    "        print('Training from beginning')\n",
    "        episode_number = 0\n",
    "        print(\"episode number:\", episode_number)\n",
    "        \n",
    "    # Starting playing\n",
    "    while episode_number<=max_episode: \n",
    "        \n",
    "        # Process observation\n",
    "        image_proc = process_obs(observation)\n",
    "        image_diff = image_proc - image_old if image_old is not None else np.zeros((82, 72))\n",
    "        image_old = image_proc\n",
    "\n",
    "        # Uniformly pick an action\n",
    "        feed_step = {image_: np.reshape(image_diff, (1,82, 72,1))}\n",
    "        aprob = sess.run(tf_aprob,feed_step) ; aprob = aprob[0,:]\n",
    "        \n",
    "        # Epsilon greedy exploration\n",
    "        ratio = np.max((max_episode - episode_number)/max_episode, 0)\n",
    "        exploration_prob = (initial_exp - final_exp) * ratio + final_exp\n",
    "        if random.random() < exploration_prob:\n",
    "            action = random.randint(0, num_actions-1)\n",
    "        else:\n",
    "            action = action = np.random.choice(num_actions, p=aprob)\n",
    "        \n",
    "        # Generate fake label for back prop\n",
    "        label = np.zeros_like(aprob) ; label[action] = 1\n",
    "\n",
    "        # Input action to OpenAI and get feedback\n",
    "        observation, reward, done, info = env.step(action+1)\n",
    "        reward_episode += reward\n",
    "\n",
    "        # Record for training\n",
    "        images.append(image_diff); fake_labels.append(label); rewards_std.append(reward); action_hist.append(action+1)\n",
    "\n",
    "        # Training\n",
    "        if done:\n",
    "            \n",
    "            if episode_number % train_every == 0:\n",
    "                \n",
    "                feed_episode = {image_: np.array(images).reshape(-1, 82, 72, 1), \n",
    "                        fake_label_: np.array(fake_labels).reshape(-1,num_actions), \n",
    "                        reward_: np.array([i * reward_factor for i in rewards_std]).reshape(-1,1)}\n",
    "                sess.run(train_op,feed_episode)\n",
    "                    \n",
    "                # Clear memory\n",
    "                episode_duration = len(rewards_std)\n",
    "                images, fake_labels, rewards_std, action_hist = [], [], [], []\n",
    "            \n",
    "            # Bookkeeping\n",
    "            reward_hist.append(reward_episode)\n",
    "            reward_runn.append(reward_episode)\n",
    "            episode_number += 1\n",
    "            observation = env.reset()\n",
    "            print('\\tep {}: reward: {} duration: {}'.format(episode_number, reward_episode,episode_duration))\n",
    "            reward_episode = 0\n",
    "            \n",
    "            # Save model\n",
    "            if episode_number % save_every == 0:\n",
    "                saver.save(sess, 'PG-Breakout-ckpt-1/Breakout.ckpt', global_step=episode_number)\n",
    "                print('Model saved, mean reward is',np.mean(reward_runn))\n",
    "                pickle.dump(episode_number,open('PG-Breakout-ckpt-1/last-episode.p','wb'))\n",
    "                pickle.dump(reward_hist,open('PG-Breakout-ckpt-1/all_reward.p','wb'))\n",
    "                plt.figure(figsize=(18,3))\n",
    "                plt.plot([7]*(len(reward_hist)-55000),'r--')\n",
    "                plt.plot(np.array(get_running_mean(reward_hist[55000:episode_number])))\n",
    "                plt.show()\n",
    "                reward_runn = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "colors = cm.rainbow(np.linspace(0, 1, 20))\n",
    "interval = 1000\n",
    "\n",
    "def get_mean_var(reward_hist):\n",
    "    ts = pd.Series(reward_hist)\n",
    "    return pd.rolling_mean(ts, interval), pd.rolling_var(ts, interval)\n",
    "\n",
    "running_mean, running_var = get_mean_var(reward_hist)\n",
    "\n",
    "plt.title('Breakout result',fontsize=20)\n",
    "plt.ylim(0,60)\n",
    "plt.plot(np.array(reward_hist),label='All rewards',c=colors[3,:])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rewards')\n",
    "plt.plot(np.array(running_mean),label='Running mean',c='y')\n",
    "plt.plot([7]*len(running_mean),'r--')\n",
    "plt.legend(loc=1,fontsize=10)\n",
    "plt.savefig('PG-Breakout-ckpt-1/ANN1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
